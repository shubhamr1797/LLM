{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.2\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.4 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trl\n",
      "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[K     |████████████████████████████████| 251 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[K     |████████████████████████████████| 314 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: datasets in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (2.7.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 105.0 MB 510 bytes/s a 0:00:01    |████                            | 13.0 MB 10.3 MB/s eta 0:00:09     |████████▍                       | 27.4 MB 11.6 MB/s eta 0:00:07     |█████████                       | 29.5 MB 11.6 MB/s eta 0:00:07     |█████████████▍                  | 44.0 MB 10.7 MB/s eta 0:00:06     |█████████████████████████▊      | 84.4 MB 2.7 MB/s eta 0:00:08\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (0.23.2)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (2022.3.2)\n",
      "Requirement already satisfied: requests in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (4.66.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (0.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from transformers==4.37.2) (3.6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from trl) (1.12.1)\n",
      "Collecting tyro>=0.5.11\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from peft) (5.9.1)\n",
      "Collecting torch>=1.4.0\n",
      "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 150.8 MB 15.6 MB/s eta 0:00:01   |███▋                            | 17.2 MB 7.8 MB/s eta 0:00:18     |████████▏                       | 38.2 MB 7.1 MB/s eta 0:00:16     |███████████████▋                | 73.6 MB 5.8 MB/s eta 0:00:14     |█████████████████████▍          | 101.0 MB 11.0 MB/s eta 0:00:05     |████████████████████████████▌   | 134.4 MB 7.8 MB/s eta 0:00:03     |████████████████████████████▉   | 135.8 MB 7.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: xxhash in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pandas in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from bitsandbytes) (1.9.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from requests->transformers==4.37.2) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from requests->transformers==4.37.2) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from requests->transformers==4.37.2) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from requests->transformers==4.37.2) (2.0.12)\n",
      "Requirement already satisfied: sympy in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from torch>=1.4.0->trl) (3.0.3)\n",
      "Requirement already satisfied: networkx in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: rich>=11.1.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from tyro>=0.5.11->trl) (12.6.0)\n",
      "Collecting docstring-parser>=0.16\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting eval-type-backport>=0.1.3\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.13.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (0.9.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shubham.rao/anaconda3/envs/auto_tuning_test/lib/python3.9/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Installing collected packages: torch, tokenizers, eval-type-backport, docstring-parser, tyro, transformers, accelerate, trl, peft, bitsandbytes\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.1\n",
      "    Uninstalling transformers-4.41.1:\n",
      "      Successfully uninstalled transformers-4.41.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1 requires torch==1.12.1, but you have torch 2.2.2 which is incompatible.\n",
      "bertopic 0.11.0 requires pyyaml<6.0, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed accelerate-0.32.1 bitsandbytes-0.42.0 docstring-parser-0.16 eval-type-backport-0.2.0 peft-0.11.1 tokenizers-0.15.2 torch-2.2.2 transformers-4.37.2 trl-0.9.6 tyro-0.8.5\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.37.2 trl peft accelerate datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = {\n",
    "            \"model_ckpt\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_lora\": True,\n",
    "            \"r\": 16,\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"task_type\": \"CAUSAL_LM\",\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "            \"output_dir\": \"sft-tiny-chatbot\",\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"optim\": \"paged_adamw_32bit\",\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"save_strategy\": \"epoch\",\n",
    "            \"logging_steps\": 100,\n",
    "            \"num_train_epochs\": 1,\n",
    "            \"max_steps\": 450,\n",
    "            \"fp16\": True,\n",
    "            \"push_to_hub\": True,\n",
    "            \"train_cln_name\": \"text\",\n",
    "            \"packing\": False,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"neftune_noise_alpha\": 5\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSFT:\n",
    "\n",
    "    def __init__(self, data, config):\n",
    "        self.data = data\n",
    "        self.config = config\n",
    "\n",
    "    def prepare_lora_model(self):\n",
    "\n",
    "        self.lora_config = LoraConfig(\n",
    "                                    r=self.config[\"r\"],\n",
    "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
    "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
    "                                    bias=self.config[\"bias\"],\n",
    "                                    task_type=self.config[\"task_type\"],\n",
    "                                    target_modules=self.config[\"target_modules\"]\n",
    "                                )\n",
    "        self.model = get_peft_model(self.model, self.lora_config)\n",
    "\n",
    "    def load_model_tokenizer(self):\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                            self.config[\"model_ckpt\"],\n",
    "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
    "                            device_map=self.config[\"device_map\"],\n",
    "                            torch_dtype=self.config[\"torch_dtype\"]\n",
    "                        )\n",
    "        self.model.config.use_cache=False\n",
    "        self.model.config.pretraining_tp=1\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        if self.config[\"use_lora\"]:\n",
    "            self.prepare_lora_model()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def set_training_args(self):\n",
    "\n",
    "        return TrainingArguments(\n",
    "                                    output_dir=self.config[\"output_dir\"],\n",
    "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
    "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
    "                                    optim=self.config[\"optim\"],\n",
    "                                    learning_rate=self.config[\"learning_rate\"],\n",
    "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
    "                                    save_strategy=self.config[\"save_strategy\"],\n",
    "                                    logging_steps=self.config[\"logging_steps\"],\n",
    "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
    "                                    max_steps=self.config[\"max_steps\"],\n",
    "                                    fp16=self.config[\"fp16\"],\n",
    "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
    "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
    "                                )\n",
    "\n",
    "    def create_trainer(self):\n",
    "\n",
    "        self.load_model_tokenizer()\n",
    "        if self.config[\"use_lora\"]:\n",
    "            print(self.model.print_trainable_parameters())\n",
    "            self.trainer = SFTTrainer(\n",
    "                                    model=self.model,\n",
    "                                    train_dataset=self.data,\n",
    "                                    peft_config=self.lora_config,\n",
    "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
    "                                    args=self.set_training_args(),\n",
    "                                    tokenizer=self.tokenizer,\n",
    "                                    packing=self.config[\"packing\"],\n",
    "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
    "                                )\n",
    "        else:\n",
    "            self.trainer = SFTTrainer(\n",
    "                                    model=self.model,\n",
    "                                    train_dataset=self.data,\n",
    "                                    dataset_text_field=self.config[\"train_cln_name\"],\n",
    "                                    args=self.set_training_args(),\n",
    "                                    tokenizer=self.tokenizer,\n",
    "                                    packing=self.config[\"packing\"],\n",
    "                                    max_seq_length=self.config[\"max_seq_length\"]\n",
    "                                )\n",
    "\n",
    "    def train_and_save_model(self):\n",
    "        self.create_trainer()\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model(self.config[\"output_dir\"])\n",
    "        self.tokenizer.save_pretrained(self.config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    data_df = data.to_pandas()\n",
    "    data_df = data_df[:700]\n",
    "    data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" Assistant: \"+ x[\"output\"], axis=1)\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data\n",
    "\n",
    "data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sft = TrainSFT(data, sft_config)\n",
    "train_sft.train_and_save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_tuning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
